{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NeuralNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOY5DktX/i4TZfqb6OjMIQd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x2iYiwTn5Bag"},"source":["# Neural Network Word-Level Generator\n","\n"]},{"cell_type":"code","metadata":{"id":"bPq1KQKu4kcU","executionInfo":{"status":"ok","timestamp":1602863054416,"user_tz":240,"elapsed":41356,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}},"outputId":"5bc25180-0d0a-43f1-bd19-7d6d5430cd43","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DPx5HUY84rSf","executionInfo":{"status":"ok","timestamp":1602863057546,"user_tz":240,"elapsed":498,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}},"outputId":"87e34543-676b-40a4-8685-651874af26ec","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd ..\n","%cd gdrive/My\\ Drive/NLP\\ Final\\ Project"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/\n","/gdrive/My Drive/NLP Final Project\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uUWzEGpFDTYG"},"source":["# Generate the Sequences"]},{"cell_type":"code","metadata":{"id":"uUpssq5Z5dgc","executionInfo":{"status":"ok","timestamp":1602455374962,"user_tz":240,"elapsed":2617,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}},"outputId":"b0ee00f0-8630-4c6e-efc6-f8fa668152b5","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import string\n","# Generates a file to hold sequences of text\n","# Don't need to run this again\n","def sequences(file):\n","  train = open(file, 'r')\n","  text = train.read()\n","  train.close()\n","\n","  # test = text.replace(' --', '')\n","\n","  words = text.split()\n","\n","  # table = str.maketrans('', '', string.punctuation)\n","  # words = [w.translate(table) for w in words]\n","\n","  length = 51\n","  sequences = list()\n","  for i in range(length, len(words)):\n","    seq = words[i-length: i]\n","    line = ' '.join(seq)\n","    sequences.append(line)\n","  \n","  doc = '\\n'.join(sequences)\n","  with open('obama_sequences.txt', 'w') as output:\n","    output.write(doc)\n","    print('making doc')\n","\n","sequences('train.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["making doc\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MP3t48FDDZYE"},"source":["# Train the Neural Model"]},{"cell_type":"code","metadata":{"id":"Z2UrYHjdztvG","executionInfo":{"status":"ok","timestamp":1602863074074,"user_tz":240,"elapsed":982,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}}},"source":["# Open the sequences doc\n","def open_doc(file):\n","  doc = open(file, 'r')\n","  text = doc.read()\n","  doc.close()\n","  return text"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrDKooJpDLs5","executionInfo":{"status":"ok","timestamp":1602463886270,"user_tz":240,"elapsed":8470228,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}},"outputId":"5f3d255e-0a2d-40ff-b960-1d8b9ab820ed","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from numpy import array\n","from pickle import dump\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","\n","\n","# open the file and split into lines\n","file = open_doc('obama_sequences.txt')\n","lines = file.split('\\n')\n","\n","# tokenize and fit the model, then turn the lines into sequences\n","tokenizer = Tokenizer(filters=' ')\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","\n","# vocab_size is length of the tokens (accessed by tokenizer.word_index) plus one due to weird indexing\n","vocab_size = len(tokenizer.word_index)+1\n","\n","# separate the sequences into input an output\n","sequences = array(sequences)\n","# X is input sequence (first 50 words), y is output word(51st word)\n","X, y = sequences[:,:-1], sequences[:,-1]\n","# one hot encodes the output word (ie 51st word)\n","y = to_categorical(y, num_classes=vocab_size)\n","seq_length = X.shape[1]\n","\n","# define the model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 50, input_length=seq_length))\n","model.add(LSTM(100, return_sequences=True))\n","model.add(LSTM(100))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(vocab_size, activation='softmax'))\n","# print(model.summary())\n","\n","# compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit model\n","model.fit(X, y, batch_size=128, epochs=90)\n","\n","model.save('model-90.h5')\n","dump(tokenizer, open('tokenizer-90.pkl', 'wb'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/90\n","447/447 [==============================] - 94s 210ms/step - loss: 6.4987 - accuracy: 0.0486\n","Epoch 2/90\n","447/447 [==============================] - 89s 200ms/step - loss: 6.0571 - accuracy: 0.0690\n","Epoch 3/90\n","447/447 [==============================] - 93s 208ms/step - loss: 5.8383 - accuracy: 0.0792\n","Epoch 4/90\n","447/447 [==============================] - 94s 210ms/step - loss: 5.6613 - accuracy: 0.0872\n","Epoch 5/90\n","447/447 [==============================] - 93s 207ms/step - loss: 5.5113 - accuracy: 0.1007\n","Epoch 6/90\n","447/447 [==============================] - 90s 202ms/step - loss: 5.3872 - accuracy: 0.1128\n","Epoch 7/90\n","447/447 [==============================] - 94s 211ms/step - loss: 5.2829 - accuracy: 0.1222\n","Epoch 8/90\n","447/447 [==============================] - 94s 210ms/step - loss: 5.1875 - accuracy: 0.1284\n","Epoch 9/90\n","447/447 [==============================] - 103s 231ms/step - loss: 5.1016 - accuracy: 0.1353\n","Epoch 10/90\n","447/447 [==============================] - 102s 229ms/step - loss: 5.0181 - accuracy: 0.1412\n","Epoch 11/90\n","447/447 [==============================] - 92s 205ms/step - loss: 4.9379 - accuracy: 0.1460\n","Epoch 12/90\n","447/447 [==============================] - 92s 205ms/step - loss: 4.8615 - accuracy: 0.1511\n","Epoch 13/90\n","447/447 [==============================] - 90s 201ms/step - loss: 4.7871 - accuracy: 0.1546\n","Epoch 14/90\n","447/447 [==============================] - 98s 219ms/step - loss: 4.7150 - accuracy: 0.1599\n","Epoch 15/90\n","447/447 [==============================] - 99s 221ms/step - loss: 4.6433 - accuracy: 0.1633\n","Epoch 16/90\n","447/447 [==============================] - 96s 214ms/step - loss: 4.5750 - accuracy: 0.1671\n","Epoch 17/90\n","447/447 [==============================] - 96s 215ms/step - loss: 4.5093 - accuracy: 0.1709\n","Epoch 18/90\n","447/447 [==============================] - 93s 208ms/step - loss: 4.4436 - accuracy: 0.1748\n","Epoch 19/90\n","447/447 [==============================] - 90s 202ms/step - loss: 4.3805 - accuracy: 0.1777\n","Epoch 20/90\n","447/447 [==============================] - 92s 205ms/step - loss: 4.3226 - accuracy: 0.1819\n","Epoch 21/90\n","447/447 [==============================] - 91s 203ms/step - loss: 4.2634 - accuracy: 0.1851\n","Epoch 22/90\n","447/447 [==============================] - 84s 187ms/step - loss: 4.2088 - accuracy: 0.1880\n","Epoch 23/90\n","447/447 [==============================] - 86s 192ms/step - loss: 4.1563 - accuracy: 0.1921\n","Epoch 24/90\n","447/447 [==============================] - 88s 197ms/step - loss: 4.1031 - accuracy: 0.1973\n","Epoch 25/90\n","447/447 [==============================] - 90s 200ms/step - loss: 4.0531 - accuracy: 0.2019\n","Epoch 26/90\n","447/447 [==============================] - 86s 193ms/step - loss: 4.0059 - accuracy: 0.2056\n","Epoch 27/90\n","447/447 [==============================] - 79s 177ms/step - loss: 3.9576 - accuracy: 0.2096\n","Epoch 28/90\n","447/447 [==============================] - 72s 160ms/step - loss: 3.9136 - accuracy: 0.2138\n","Epoch 29/90\n","447/447 [==============================] - 67s 151ms/step - loss: 3.8701 - accuracy: 0.2188\n","Epoch 30/90\n","447/447 [==============================] - 67s 151ms/step - loss: 3.8261 - accuracy: 0.2220\n","Epoch 31/90\n","447/447 [==============================] - 84s 189ms/step - loss: 3.7877 - accuracy: 0.2273\n","Epoch 32/90\n","447/447 [==============================] - 91s 203ms/step - loss: 3.7485 - accuracy: 0.2317\n","Epoch 33/90\n","447/447 [==============================] - 84s 188ms/step - loss: 3.7115 - accuracy: 0.2355\n","Epoch 34/90\n","447/447 [==============================] - 69s 155ms/step - loss: 3.6759 - accuracy: 0.2410\n","Epoch 35/90\n","447/447 [==============================] - 76s 170ms/step - loss: 3.6400 - accuracy: 0.2439\n","Epoch 36/90\n","447/447 [==============================] - 77s 172ms/step - loss: 3.6064 - accuracy: 0.2498\n","Epoch 37/90\n","447/447 [==============================] - 79s 177ms/step - loss: 3.5704 - accuracy: 0.2544\n","Epoch 38/90\n","447/447 [==============================] - 84s 187ms/step - loss: 3.5418 - accuracy: 0.2566\n","Epoch 39/90\n","447/447 [==============================] - 86s 192ms/step - loss: 3.5097 - accuracy: 0.2626\n","Epoch 40/90\n","447/447 [==============================] - 81s 182ms/step - loss: 3.4809 - accuracy: 0.2645\n","Epoch 41/90\n","447/447 [==============================] - 76s 171ms/step - loss: 3.4525 - accuracy: 0.2697\n","Epoch 42/90\n","447/447 [==============================] - 73s 163ms/step - loss: 3.4244 - accuracy: 0.2731\n","Epoch 43/90\n","447/447 [==============================] - 75s 169ms/step - loss: 3.3958 - accuracy: 0.2760\n","Epoch 44/90\n","447/447 [==============================] - 80s 178ms/step - loss: 4.2280 - accuracy: 0.2093\n","Epoch 45/90\n","447/447 [==============================] - 85s 190ms/step - loss: 3.9444 - accuracy: 0.2097\n","Epoch 46/90\n","447/447 [==============================] - 93s 209ms/step - loss: 3.5377 - accuracy: 0.2523\n","Epoch 47/90\n","447/447 [==============================] - 89s 199ms/step - loss: 3.4080 - accuracy: 0.2721\n","Epoch 48/90\n","447/447 [==============================] - 89s 199ms/step - loss: 3.3474 - accuracy: 0.2835\n","Epoch 49/90\n","447/447 [==============================] - 92s 206ms/step - loss: 3.3069 - accuracy: 0.2912\n","Epoch 50/90\n","447/447 [==============================] - 106s 237ms/step - loss: 3.2797 - accuracy: 0.2945\n","Epoch 51/90\n","447/447 [==============================] - 105s 235ms/step - loss: 3.2565 - accuracy: 0.2963\n","Epoch 52/90\n","447/447 [==============================] - 97s 218ms/step - loss: 3.2351 - accuracy: 0.3004\n","Epoch 53/90\n","447/447 [==============================] - 96s 214ms/step - loss: 3.2140 - accuracy: 0.3031\n","Epoch 54/90\n","447/447 [==============================] - 97s 217ms/step - loss: 3.1933 - accuracy: 0.3080\n","Epoch 55/90\n","447/447 [==============================] - 102s 229ms/step - loss: 3.1731 - accuracy: 0.3099\n","Epoch 56/90\n","447/447 [==============================] - 103s 230ms/step - loss: 3.1533 - accuracy: 0.3141\n","Epoch 57/90\n","447/447 [==============================] - 102s 228ms/step - loss: 3.1312 - accuracy: 0.3167\n","Epoch 58/90\n","447/447 [==============================] - 100s 223ms/step - loss: 3.1103 - accuracy: 0.3188\n","Epoch 59/90\n","447/447 [==============================] - 98s 220ms/step - loss: 3.0890 - accuracy: 0.3225\n","Epoch 60/90\n","447/447 [==============================] - 93s 209ms/step - loss: 3.0664 - accuracy: 0.3263\n","Epoch 61/90\n","447/447 [==============================] - 98s 220ms/step - loss: 3.0450 - accuracy: 0.3293\n","Epoch 62/90\n","447/447 [==============================] - 97s 217ms/step - loss: 3.0224 - accuracy: 0.3336\n","Epoch 63/90\n","447/447 [==============================] - 94s 211ms/step - loss: 2.9957 - accuracy: 0.3381\n","Epoch 64/90\n","447/447 [==============================] - 94s 211ms/step - loss: 2.9734 - accuracy: 0.3416\n","Epoch 65/90\n","447/447 [==============================] - 99s 221ms/step - loss: 2.9575 - accuracy: 0.3431\n","Epoch 66/90\n","447/447 [==============================] - 103s 230ms/step - loss: 2.9334 - accuracy: 0.3462\n","Epoch 67/90\n","447/447 [==============================] - 106s 238ms/step - loss: 2.9109 - accuracy: 0.3516\n","Epoch 68/90\n","447/447 [==============================] - 103s 230ms/step - loss: 2.8880 - accuracy: 0.3537\n","Epoch 69/90\n","447/447 [==============================] - 101s 226ms/step - loss: 2.8695 - accuracy: 0.3567\n","Epoch 70/90\n","447/447 [==============================] - 111s 248ms/step - loss: 2.8504 - accuracy: 0.3608\n","Epoch 71/90\n","447/447 [==============================] - 112s 250ms/step - loss: 2.8249 - accuracy: 0.3643\n","Epoch 72/90\n","447/447 [==============================] - 96s 215ms/step - loss: 2.8065 - accuracy: 0.3694\n","Epoch 73/90\n","447/447 [==============================] - 96s 214ms/step - loss: 2.7859 - accuracy: 0.3698\n","Epoch 74/90\n","447/447 [==============================] - 99s 221ms/step - loss: 2.7624 - accuracy: 0.3775\n","Epoch 75/90\n","447/447 [==============================] - 98s 220ms/step - loss: 2.7420 - accuracy: 0.3785\n","Epoch 76/90\n","447/447 [==============================] - 99s 222ms/step - loss: 2.7233 - accuracy: 0.3828\n","Epoch 77/90\n","447/447 [==============================] - 102s 228ms/step - loss: 2.7056 - accuracy: 0.3837\n","Epoch 78/90\n","447/447 [==============================] - 106s 238ms/step - loss: 2.6841 - accuracy: 0.3907\n","Epoch 79/90\n","447/447 [==============================] - 105s 235ms/step - loss: 2.6654 - accuracy: 0.3930\n","Epoch 80/90\n","447/447 [==============================] - 104s 233ms/step - loss: 2.6441 - accuracy: 0.3968\n","Epoch 81/90\n","447/447 [==============================] - 104s 232ms/step - loss: 2.6334 - accuracy: 0.3974\n","Epoch 82/90\n","447/447 [==============================] - 104s 234ms/step - loss: 2.6125 - accuracy: 0.4010\n","Epoch 83/90\n","447/447 [==============================] - 104s 233ms/step - loss: 2.5951 - accuracy: 0.4052\n","Epoch 84/90\n","447/447 [==============================] - 104s 232ms/step - loss: 2.5751 - accuracy: 0.4076\n","Epoch 85/90\n","447/447 [==============================] - 106s 236ms/step - loss: 2.5551 - accuracy: 0.4118\n","Epoch 86/90\n","447/447 [==============================] - 108s 242ms/step - loss: 2.5366 - accuracy: 0.4176\n","Epoch 87/90\n","447/447 [==============================] - 107s 240ms/step - loss: 2.5196 - accuracy: 0.4189\n","Epoch 88/90\n","447/447 [==============================] - 107s 239ms/step - loss: 2.5028 - accuracy: 0.4217\n","Epoch 89/90\n","447/447 [==============================] - 106s 237ms/step - loss: 2.4879 - accuracy: 0.4237\n","Epoch 90/90\n","447/447 [==============================] - 113s 253ms/step - loss: 2.4754 - accuracy: 0.4254\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QyPSxIcFzUDx","executionInfo":{"status":"ok","timestamp":1602863097454,"user_tz":240,"elapsed":15567,"user":{"displayName":"Delaney Fowler","photoUrl":"","userId":"15472196158022706169"}},"outputId":"fa56d067-5ae5-4660-c217-644d4c793f26","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["from random import randint\n","from pickle import load\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","\n","def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n","  result = list()\n","  input = seed_text\n","  # generate given num words\n","  for _ in range(n_words):\n","    # Encode seed text\n","    encoded = tokenizer.texts_to_sequences([input])[0]\n","    # Truncate sequences to length of seq\n","    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n","    # Predict probs for each word\n","    probs = model.predict_classes(encoded, verbose=0)\n","    # Map predicted word index to a word\n","    out_word = ''\n","    for word, index in tokenizer.word_index.items():\n","      if index == probs:\n","        out_word = word\n","        break\n","\n","    # Append to input\n","    input += ' ' + out_word\n","    result.append(out_word)\n","\n","  return ' '.join(result)\n","\n","\n","# open the file and split into lines\n","file = open_doc('obama_sequences.txt')\n","lines = file.split('\\n')\n","\n","seq_length = len(lines[0].split()) - 1\n","\n","# Load model and tokenizer for text generation\n","model = load_model('model-90.h5')\n","tokenizer = load(open('tokenizer-90.pkl', 'rb'))\n","\n","for _ in range(5):\n","  # Select a seed\n","  seed_text = lines[randint(0, len(lines))]\n","  print('Seed: ' + seed_text + '\\n')\n","\n","  generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n","  print(generated + '\\n')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Seed: america . mr. speaker , mr. vice president , members of congress , distinguished guests , and fellow americans : last month , i went to andrews air force base and welcomed home some of our last troops to serve in iraq . together , we offered a final , proud\n","\n","WARNING:tensorflow:From <ipython-input-4-684f315c182d>:16: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n","Instructions for updating:\n","Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","salute to the polls who believe that we can do it . and we need to teach it with the next 5 years . we should be able to do business . we can do it . but we can do it if we want to work with the world\n","\n","Seed: reducing teen pregnancy , even reducing violent crime . in states that make it a priority to educate our youngest children , like georgia or oklahoma , studies show students grow up more likely to read and do math at grade level , graduate high school , hold a job ,\n","\n","form us broken affordable for our shores . we should be able to do business . we can do it . but we can do it if we want to work with the world . we should be able to do business . we can do it . but we\n","\n","Seed: a president , but as a father , when i say that responsibility for our children's education must begin at home . that is not a democratic issue or a republican issue; that's an american issue . there is , of course , another responsibility we have to our children .\n","\n","and that's why we can do it . but we can do it if we want to work with the world . we should be able to do business . we can do it . but we can do it if we want to work with the world . we\n","\n","Seed: right now and will be there in the future . now , even with better high schools , most young people will need some higher education . but today , skyrocketing costs price too many young people out of a higher education or saddle them with unsustainable debt . through tax\n","\n","credits , grants , and give the spread of poverty and teacher afghan kids for a new foundation . neither party means establishing a nuclear weapon . and that's why we can do it if we want to work with the world . we should be able to do business\n","\n","Seed: tonight . now , kentucky is not the most liberal part of the country . that's not where i got my highest vote totals . [laughter] but he's like a man possessed when it comes to covering his commonwealth's families . they're our neighbors and our friends , he said :\n","\n","i know that we can do it . but we can do it if we want to work with the world . we should be able to do business . we can do it . but we can do it if we want to work with the world . we\n","\n"],"name":"stdout"}]}]}