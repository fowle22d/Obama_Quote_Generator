{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluation.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JkeWGSg2mhun"},"source":["def leven_dist_char(source, target):\n","    \"\"\"\n","    *find minimum edit distance between two strings\n","    *character edit distance\n","    \"\"\"\n","    ### TODO: write code for function here\n","    from numpy import zeros\n","\n","    #print(source+\"-\"+target)\n","\n","    #get length of strings\n","    s_len = len(source)\n","    t_len = len(target)\n","\n","    #create grid\n","    grid = zeros((s_len+1, t_len+1), dtype=int)\n","\n","    #initialize grid\n","    for i in range(s_len+1):\n","      grid[i][0] = i\n","\n","    for j in range(t_len+1):\n","      grid[0][j] = j\n","\n","    #recurrence relation\n","    for i in range(1, s_len+1):\n","      for j in range(1, t_len+1):\n","        \n","        #get characters\n","        s_char = source[i-1]\n","        t_char = target[j-1]\n","\n","        #determine min cost\n","        delt= grid[i-1][j] + 1 #deletion = cost 1\n","        inst= grid[i][j-1] + 1 #insertion = cost 1\n","\n","        #if characters are the same, substution = cost 0\n","        if s_char==t_char:\n","          subst = grid[i-1][j-1]\n","        #if characters are different, substution = cost 2\n","        else:\n","          subst = grid[i-1][j-1] + 2\n","        \n","        #fill grid with minimum cost operation\n","        grid[i][j] = min(delt, inst, subst)\n","\n","\n","    #print final grid\n","    #print(grid)\n","\n","    #distance is grid[s][t]\n","    dist = grid[s_len][t_len]\n","    #print distance\n","    #print(\"Levenshtein distance is: \"+str(dist))\n","\n","    #return distance\n","    return dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FV-lhEdOmtNi"},"source":["### run this to test your function\n","leven_dist_char('this is her box .', 'we are cool .')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRPy1WzsFkht"},"source":["def leven_dist_words(source, target):\n","    \"\"\"\n","    *find minimum edit distance between two strings\n","    *word edit distance\n","    \"\"\"\n","    ### TODO: write code for function here\n","    from numpy import zeros\n","\n","    #print(source+\" - \"+target)\n","\n","    #create word lists\n","    s_list = source.split()\n","    t_list = target.split()\n","    \n","    #get length of strings\n","    s_len = len(s_list)\n","    t_len = len(t_list)\n","\n","    #create grid\n","    grid = zeros((s_len+1, t_len+1), dtype=int)\n","\n","    #initialize grid\n","    for i in range(s_len+1):\n","      grid[i][0] = i\n","\n","    for j in range(t_len+1):\n","      grid[0][j] = j\n","\n","    #recurrence relation\n","    for i in range(1, s_len+1):\n","      for j in range(1, t_len+1):\n","        \n","        #get characters\n","        s_word = s_list[i-1]\n","        t_word = t_list[j-1]\n","\n","        #determine min cost\n","        delt= grid[i-1][j] + 1 #deletion = cost 1\n","        inst= grid[i][j-1] + 1 #insertion = cost 1\n","\n","        #if characters are the same, substution = cost 0\n","        if s_word==t_word:\n","          subst = grid[i-1][j-1]\n","        #if characters are different, substution = cost 2\n","        else:\n","          subst = grid[i-1][j-1] + 2\n","        \n","        #fill grid with minimum cost operation\n","        grid[i][j] = min(delt, inst, subst)\n","\n","\n","    #print final grid\n","    #print(grid)\n","\n","    #distance is grid[s][t]\n","    dist = grid[s_len][t_len]\n","    #print distance\n","    #print(\"Levenshtein distance is: \"+str(dist))\n","\n","    #return distance\n","    return dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eeAjb7WpGEO3"},"source":["### run this to test your function\n","leven_dist_words('this is her box .', 'we are cool .')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqOffh3wJxaJ"},"source":["def eval_multi_OLD(outFile, trainFile, char):\n","  \"\"\"\n","  *determine the average minimum edit distance \n","  *between a generated line and all training lines\n","  *creates array of average distance\n","  *if line was identical to any in training, average is -1\n","  \"\"\"\n","\n","  outLines = [] #all generated lines to check\n","  outLineAvg = [] #average distance for associated line from each training quote\n","  identicalCount = 0 #how many lines were identical to a training quote\n","  trainCount = 0 #number of training quotes\n","\n","  #add generated lines to list\n","  f_out = open(outFile,encoding=\"utf8\")\n","  for line in f_out:\n","    #remove newline character\n","    line = line.replace(\"\\n\", \"\")\n","\n","    #add line and initialize average in list\n","    outLines.append(line)\n","    outLineAvg.append(0)\n","  f_out.close()\n","  \n","  #check each training line\n","  f_train = open(trainFile,encoding=\"utf8\")\n","  for trainLine in f_train:\n","    #remove newline characters\n","    trainLine = trainLine.replace(\"\\n\", \"\")\n","    #increase train quote count\n","    trainCount += 1\n","\n","    #for each generated line\n","    for idx in range(len(outLines)):\n","      outLine = outLines[idx]\n","\n","      #calculate char or word distance\n","      if char:\n","        dist = leven_dist_char(trainLine, outLine)\n","      else:\n","        dist = leven_dist_words(trainLine, outLine)\n","        \n","      #add distanct to average of it's not previously identical\n","      if outLineAvg[idx] != -1:\n","        #if identical, make -1\n","        if dist <= 10:\n","          outLineAvg[idx] = -1\n","        else:\n","          outLineAvg[idx] += dist \n","\n","  f_train.close()\n","\n","  #calculate average for each line unless identical\n","  for i in range(len(outLines)):\n","    if outLineAvg[i] != -1:\n","      outLineAvg[i] = outLineAvg[i]/trainCount\n","    \n","    #print(outLines[i]+\" --- distance: \"+str(outLineAvg[i]))\n","  print(outLineAvg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BL_XzXme-2MU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83VojA3i-2nm"},"source":["def eval_multi(outFile, trainFile, char):\n","  \"\"\"\n","  *determine the minimum minimum edit distance \n","  *between a generated line and all training lines\n","  *creates array of min distance\n","  *if line was identical to any in training, average is -1\n","  \"\"\"\n","\n","  outLines = [] #all generated lines to check\n","  outLineAvg = [] #average distance for associated line from each training quote\n","  identicalCount = 0 #how many lines were identical to a training quote\n","  trainCount = 0 #number of training quotes\n","\n","  #add generated lines to list\n","  f_out = open(outFile,encoding=\"utf8\")\n","  for line in f_out:\n","    #remove newline character\n","    line = line.replace(\"\\n\", \"\")\n","\n","    #add line and initialize average in list\n","    outLines.append(line)\n","    outLineAvg.append(100000000)\n","  f_out.close()\n","  \n","  #check each training line\n","  f_train = open(trainFile,encoding=\"utf8\")\n","  for trainLine in f_train:\n","    #remove newline characters\n","    trainLine = trainLine.replace(\"\\n\", \"\")\n","    #increase train quote count\n","    trainCount += 1\n","\n","    #for each generated line\n","    for idx in range(len(outLines)):\n","      outLine = outLines[idx]\n","\n","      #calculate char or word distance\n","      if char:\n","        dist = leven_dist_char(trainLine, outLine)\n","      else:\n","        dist = leven_dist_words(trainLine, outLine)\n","        \n","      #add distanct to average of it's not previously identical\n","      if dist < outLineAvg[idx]:\n","        outLineAvg[idx] = dist\n","      # if outLineAvg[idx] != -1:\n","      #   #if identical, make -1\n","      #   if dist <= 10:\n","      #     outLineAvg[idx] = -1\n","      #   else:\n","      #     outLineAvg[idx] += dist \n","\n","  f_train.close()\n","\n","  #calculate average for each line unless identical\n","  # for i in range(len(outLines)):\n","  #   if outLineAvg[i] != -1:\n","  #     outLineAvg[i] = outLineAvg[i]/trainCount\n","    \n","    #print(outLines[i]+\" --- distance: \"+str(outLineAvg[i]))\n","  for i in range(len(outLines)):\n","    print(outLines[i])\n","    print(outLineAvg[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QybAr4UgSrna"},"source":["eval_multi(\"tinytest1generated.txt\", \"tinytest1.txt\", False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtsehIfc11Di","executionInfo":{"status":"ok","timestamp":1602542969758,"user_tz":240,"elapsed":26674,"user":{"displayName":"Rebekah McBane","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZlU6wJcEIWoYC3eX2zf_YoVsU--cjEnoG0P3q=s64","userId":"12820766840184894859"}},"outputId":"33fb4c22-25be-4d55-df2b-6646436f6a9f","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["eval_multi(\"generated_text_both.txt\", \"train.txt\", False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[18, 29, 11, 23, 37, 14, 0, 13, 5, 13]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UcBAtuV_GG8q"},"source":["# Bigram Generated Sentences\n","\n","| phrase id | phrase\n","--- | ---\n","1 | so was here in iraq to be clear : we too often isn't about how long as a difference .\n","2 | a job creation , we don't need more than 90 days when we can't bring this year , as you take a part of the most young people we do spectacularly well .\n","3 | some earmark reform right now , where we believed : they're people .\n","4 | and that's the answers to reaffirm our efforts that american people every business in a trade deals that the last 2 years , depends on .\n","5 | and suspicion between a source of this chamber that uniform stand against us to be able to send me start making higher education and combating the inherent dignity cannot be pushing for the problem on this recession , vote .\n","\n","# Neural Network Generated Sentences\n","| phrase id | phrase\n","--- | ---\n","1 | and chicago , our forces is strong , we will do whatever we can do it .\n","2 | god bless you .\n","3 | we should follow our relationship with freedom and a brain cancer by our future .\n","4 | and that's why we can do it .\n","5 | and that's why we can do it if we want to work with the world ."]},{"cell_type":"markdown","metadata":{"id":"u8R-AbEuFuCB"},"source":["# Min Edit Distance of each Sentence\n","\n","| phrase id | generator | min edit distance\n","--- | --- | :---:\n","1 | bigram | 18\n","2 | bigram | 29\n","3 | bigram | 11\n","4 | bigram | 23\n","5 | bigram | 37\n","1 | NN | 14\n","2 | NN | 0\n","3 | NN | 13\n","4 | NN | 5\n","5 | NN | 13"]},{"cell_type":"markdown","metadata":{"id":"-_VqLea6HoxA"},"source":["# Model Uniqueness Averages\n","\n","| generator | uniqueness\n","--- | :---:\n","bigram | 23.6\n","NN | 9"]},{"cell_type":"markdown","metadata":{"id":"L3pRavVqrtHx"},"source":["# Num Participants\n","\n","| phrase id | generator | num participants\n","--- | --- | ---\n","1 | bigram | 17\n","2 | bigram | 14\n","3 | bigram | 17\n","4 | bigram | 14\n","5 | bigram | 17\n","1 | neural net | 14\n","2 | neural net | 17\n","3 | neural net | 14\n","4 | neural net | 17\n","5 | neural net | 14"]},{"cell_type":"markdown","metadata":{"id":"wCsHpdp3oxwn"},"source":["# Raw Numbers (not averages)\n","\n","| phrase id | generator | style | fluency | meaning |\n","--- | --- | :---: | :---: | :---: \n","1 | bigram | 39 | 20 | 18\n","2 | bigram | 35 | 22 | 21\n","3 | bigram |57 | 35 | 30\n","4 | bigram | 42 | 34 | 32\n","5 | bigram |45| 29 | 27\n","1 | neural net | 41 | 39 | 43\n","2 | neural net | 65 | 82 | 85\n","3 | neural net | 45 | 46 | 39\n","4 | neural net | 73 | 80 | 81\n","5 | neural net |56 | 58 | 65"]},{"cell_type":"markdown","metadata":{"id":"H_w0OOir9jYN"},"source":["# Average Score for each phrase\n","\n","| phrase id | generator | style | fluency | meaning |\n","--- | --- | :---: | :---: | :---: \n","1 | bigram | 2.29 | 1.18 | 1.06\n","2 | bigram | 2.5 | 1.57 | 1.5\n","3 | bigram | 3.35 | 2.06 | 1.76\n","4 | bigram | 3 | 2.43 | 2.29\n","5 | bigram | 2.65 | 1.71 | 1.59\n","1 | NN | 2.93 | 2.79 | 3.07\n","2 | NN |3.82 | 4.82 | 5\n","3 | NN | 3.21 | 3.29 | 2.79\n","4 | NN | 4.29| 4.71 | 4.76\n","5 | NN | 4 | 4.14 | 4.64"]},{"cell_type":"markdown","metadata":{"id":"CgUHbWqqElre"},"source":["# Generator Averages Question Features\n","\n","| generator | style | fluency | meaning |\n","--- | :---: | :---: | :---:\n","bigram | 2.76 | 1.79 | 1.64\n","NN | 3.65 | 3.95 | 4.05"]},{"cell_type":"markdown","metadata":{"id":"CawZXw7ZGbCe"},"source":["# Generator Averages Study Features\n","\n","| generator | style | readability |\n","--- | :---: | :---: \n","bigram | 2.76 | 1.71\n","NN | 3.65 | 4"]},{"cell_type":"markdown","metadata":{"id":"Xyg15_ChGzKM"},"source":["# Generator Averages Study Features (corrected for 0)\n","\n","| generator | style | readability |\n","--- | :---: | :---: \n","bigram | 2.76 | 1.71\n","NN | 2.89 | 3.02"]}]}